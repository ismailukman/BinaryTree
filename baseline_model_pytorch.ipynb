{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline_model_pytorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ismailukman/BinaryTree/blob/master/baseline_model_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "xa7h191o7_7N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## DSS images\n",
        "\n",
        "Here we'll be training a CNN from the scratch to classify the DSS images. We'll start, as usual, by importing our usual resources. And checking if we can train our model on GPU. The hyper-parameters used were obtained by training on very low resolution images, but there is a lot of scope for additional tuning.\n",
        "\n",
        "### Ensure data available\n",
        "\n",
        "If the data is not present in a specified sub-directory download from my ebi home directory\n",
        "Download the DSS data from [this link](https://www.ebi.ac.uk/~kola/low_res_images_epithelium_224x224_test_train_valid.tar.gz), save it in the home directory of this notebook and extract the tar.gz file to get the directory `low_res_images_epithelium_224x224/`. **Change the data_dir variable below to reflect this directory**."
      ]
    },
    {
      "metadata": {
        "id": "eEkq3k4v7_7O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ivav5m5b7_7S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BoKb4Edc8jls",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://www.ebi.ac.uk/~kola/low_res_images_epithelium_224x224_test_train_valid.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UsC_Vq6P8u2b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!tar -zxvf low_res_images_epithelium_224x224_test_train_valid.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hT95PFOU7_7Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load and Transform our Data\n",
        "\n",
        "We'll be using PyTorch's [ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder) class which makes is very easy to load data from a directory. For example, the training images are all stored in a directory path that looks like this:\n",
        "```\n",
        "root/class_1/xxx.png\n",
        "root/class_1/xxy.png\n",
        "root/class_1/xxz.png\n",
        "\n",
        "root/class_2/123.png\n",
        "root/class_2/nsdf3.png\n",
        "root/class_2/asd932_.png\n",
        "```\n",
        "\n",
        "Where, in this case, the root folder for training is `low_res_images_epithelium_224x224/train/` and the classes are the scores given to the images."
      ]
    },
    {
      "metadata": {
        "id": "8CE13EaO7_7Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define training and test data directories\n",
        "# THIS DIRECTORY IS WHERE YOU EXTRACTED THE IMAGES TO\n",
        "data_dir = 'low_res_images_epithelium_224x224/'\n",
        "train_dir = os.path.join(data_dir, 'train/')\n",
        "valid_dir = os.path.join(data_dir, 'valid/')\n",
        "test_dir = os.path.join(data_dir, 'test/')\n",
        "\n",
        "# classes are folders in each directory with these names\n",
        "classes = ['0.0', '0.5', '1.0', '1.5', '2.0', '3.0']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3JMD838y7_7b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Transforming the Data\n",
        "Here we are using the same set of images used for transfer learning with the VGG16 pre-trained model. When we perform transfer learning, we have to shape our input data into the shape that the pre-trained model expects. VGG16 expects 224-dim square images as input and so, we resize each image to fit this mold."
      ]
    },
    {
      "metadata": {
        "id": "Djnd_11z7_7c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load and transform data using ImageFolder\n",
        "\n",
        "# VGG-16 Takes 224x224 images as input, so we resize all of them (we will use VGG16 later)\n",
        "data_transform = transforms.Compose([transforms.RandomResizedCrop(224), \n",
        "                                      transforms.ToTensor()])\n",
        "\n",
        "train_data = datasets.ImageFolder(train_dir, transform=data_transform)\n",
        "valid_data = datasets.ImageFolder(valid_dir, transform=data_transform)\n",
        "test_data = datasets.ImageFolder(test_dir, transform=data_transform)\n",
        "\n",
        "# print out some data stats\n",
        "n_train = len(train_data)\n",
        "n_valid = len(valid_data)\n",
        "n_test = len(test_data)\n",
        "print('Num training images: ', n_train)\n",
        "print('Num validation images: ', n_valid)\n",
        "print('Num test images: ', n_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BoYrb7Dt7_7g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define dataloader parameters\n",
        "batch_size = 20\n",
        "num_workers=0\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
        "                                           num_workers=num_workers, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, \n",
        "                                           num_workers=num_workers, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
        "                                          num_workers=num_workers, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bf0w_3C17_7j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Visualize some sample data\n",
        "\n",
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "images = images.numpy() # convert images to numpy for display\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n",
        "    ax.set_title(classes[labels[idx]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ipbIt4xl7_7n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Define the Model\n",
        "\n",
        "To define a model for training we use hyperparameters obtained from training very low resolution images:<br>\n",
        "    * A few convolutional layers\n",
        "    * One fully connected layer\n",
        "    * One softmax (output) layer"
      ]
    },
    {
      "metadata": {
        "id": "wbeVnESv7_7o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
        "        nn.init.kaiming_normal_(self.conv1.weight)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
        "        nn.init.kaiming_normal_(self.conv2.weight)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3)\n",
        "        nn.init.kaiming_normal_(self.conv3.weight)\n",
        "        \n",
        "        self.drop2d = nn.Dropout2d(0.25)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        self.fc1 = nn.Linear(128 * 26 * 26, 200)\n",
        "        self.fc2 = nn.Linear(200, 6)\n",
        "        self.drop1d = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.drop2d(x)\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.drop2d(x)\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        \n",
        "        # Each unpadded 2d convolution reduces dimensions by 2, each maxpool by factor of 0.2\n",
        "        x = x.view(-1, 128 * 26 * 26) \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.drop1d(x)\n",
        "        # When using keras I applied softmax to output. Not sure what difference this makes\n",
        "        #x = F.softmax(self.fc2(x),dim=0)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "model = Net()\n",
        "print(model)\n",
        "\n",
        "# If GPU is available, move the model to GPU\n",
        "if train_on_gpu:\n",
        "    model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fm9z8ofk7_7s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# specify loss function (categorical cross-entropy)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# specify optimizer (stochastic gradient descent) and learning rate\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Use scheduler to decrease learning rate every step_size epochs\n",
        "step_size = 10\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, 20, gamma=0.5, last_epoch=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O0CObP8U7_7v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 50\n",
        "\n",
        "## TODO complete epoch and training batch loops\n",
        "## These loops should update the classifier-weights of this model\n",
        "## And track (and print out) the training loss over time\n",
        "\n",
        "for epoch in np.arange(1,n_epochs+1):\n",
        "    training_loss = 0.0\n",
        "    model.train()\n",
        "    scheduler.step()\n",
        "    for i, inputs in enumerate(train_loader):\n",
        "        data, target = inputs\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        training_loss += loss.item()\n",
        "        \n",
        "    #Apply model to validation set\n",
        "    model.eval()\n",
        "    validation_loss = 0.0\n",
        "    for i, inputs in enumerate(valid_loader):\n",
        "        data, target = inputs\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, target)\n",
        "        validation_loss += loss.item()    \n",
        "        \n",
        "    print('{0:d}, training_loss: {1:.5f}, validation_loss: {2:.5f}'.format(epoch, training_loss/n_train, validation_loss/n_valid))\n",
        "print('Finished Training')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6HdK8SWF7_70",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Testing\n",
        "\n",
        "Below you see the test accuracy for each class."
      ]
    },
    {
      "metadata": {
        "id": "UnJQmOFA7_71",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to create confusion matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "def conf_mat(row_arr,col_arr):\n",
        "    if not np.all(row_arr.shape == col_arr.shape):\n",
        "        print(\"Need shapes of both arrays to be equal\")\n",
        "        return False\n",
        "    n = row_arr.shape[-1]\n",
        "    unique_values = list(set(row_arr[:]).union(set(col_arr[:])))\n",
        "    unique_values.sort()\n",
        "    n2 = len(unique_values)\n",
        "    cmat = np.zeros((n2,n2),np.int)\n",
        "    \n",
        "    for r, c in zip(row_arr, col_arr):\n",
        "        row_index = unique_values.index(r)\n",
        "        col_index = unique_values.index(c)\n",
        "        cmat[row_index,col_index] += 1\n",
        "\n",
        "    return pd.DataFrame(data = cmat, index=unique_values, columns=unique_values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7VsGsdI27_75",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# track test loss over all 6 classes\n",
        "n_classes = 6\n",
        "test_loss = 0.0\n",
        "class_correct = list(0. for i in range(n_classes))\n",
        "class_total = list(0. for i in range(n_classes))\n",
        "\n",
        "# Also create a confusion matrix to give insight into results\n",
        "list_targets = []\n",
        "list_outputs = []\n",
        "\n",
        "# iterate over test data\n",
        "for data, target in test_loader:\n",
        "    # move tensors to GPU if CUDA is available\n",
        "    if train_on_gpu:\n",
        "        data, target = data.cuda(), target.cuda()\n",
        "    # forward pass: compute predicted outputs by passing inputs to the model\n",
        "    output = model(data)\n",
        "    # calculate the batch loss\n",
        "    loss = criterion(output, target)\n",
        "    # update  test loss \n",
        "    # test_loss += loss.item()*data.size(0) - correct way to calculate loss, but use same metric as for training and validation\n",
        "    test_loss += loss.item()\n",
        "    # convert output probabilities to predicted class\n",
        "    _, pred = torch.max(output, 1)    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    # calculate test accuracy for each object class\n",
        "    for i in range(target.data.size()[0]):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "    \n",
        "    # Add to list for calculating confusion matrix\n",
        "    list_outputs.extend(np.squeeze(pred.numpy()) if not train_on_gpu else np.squeeze(pred.cpu().numpy()))\n",
        "    list_targets.extend(np.squeeze(target.numpy()) if not train_on_gpu else np.squeeze(target.cpu().numpy()))\n",
        "\n",
        "# calculate avg test loss\n",
        "test_loss = test_loss/n_test\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "for i in range(n_classes):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            classes[i], 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))\n",
        "\n",
        "print('\\nConfusion matrix:')\n",
        "print(conf_mat(np.array(list_targets),np.array(list_outputs)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fu8THQjx7_79",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# obtain one batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = dataiter.next()\n",
        "images.numpy()\n",
        "\n",
        "# move model inputs to cuda, if GPU available\n",
        "if train_on_gpu:\n",
        "    images = images.cuda()\n",
        "\n",
        "# get sample outputs\n",
        "output = model(images)\n",
        "# convert output probabilities to predicted class\n",
        "_, preds_tensor = torch.max(output, 1)\n",
        "preds = np.squeeze(preds_tensor.numpy()) if not train_on_gpu else np.squeeze(preds_tensor.cpu().numpy())\n",
        "\n",
        "# plot the images in the batch, along with predicted and true labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    plt.imshow(np.transpose(images[idx], (1, 2, 0)))\n",
        "    ax.set_title(\"{} ({})\".format(classes[preds[idx]], classes[labels[idx]]),\n",
        "                 color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UMl8cEm87_8A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IqiaL8EA8btE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}